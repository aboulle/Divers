{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook présente une implémentation pas à pas d'un réseau neurones en NumPy destiné à reconnaitre des chiffres manuscrits. Le réseau peut contenir un nombre arbitraire de couches et de neurones par couche. Les calculs sont entièrements vectorisés (implémentation adaptée de http://neuralnetworksanddeeplearning.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps chargeons les données [MNIST](http://yann.lecun.com/exdb/mnist/). Celles-ci sont des images de 28x28 pixels représentant des chiffres manuscrits et leur identification. Cette base de donnée provient du NIST. La représentation utilisée ici a été préparée pour être directement lue avec Python par le laboratoire de Machine Learning de l'université de Montreal: http://www.deeplearning.net/tutorial/gettingstarted.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> <class 'tuple'> <class 'tuple'>\n",
      "(50000, 784) (50000,) (10000, 784) (10000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "#Affiche le type et les dimensions des données\n",
    "print(type(training_data), type(validation_data), type(test_data))\n",
    "print(np.shape(training_data[0]), np.shape(training_data[1]),\n",
    "     np.shape(validation_data[0]), np.shape(validation_data[1]),\n",
    "     np.shape(test_data[0]), np.shape(test_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lecture de ce fichier renvoie trois tuples ``(x,y)``. ``x`` est une matrice 50 000x784 (ou 10 000x784) où 50 000 (ou 10 000) est le nombre d'images et 784 est le vecteur correspondant à l'image (28x28 = 784 pixels). ``y`` est un vecteur de 50 000 lignes (ou 10 000 lignes) correspondant au chiffre affiché dans chaque image.\n",
    "\n",
    "Re-organisons les données pour qu'elles soient plus facilement manipulables par la suite. Le tableau ``training_data_x`` sera un tableau 50 000x784. Chacune des 50 000 cases de ce tableau contient le vecteur image``x`` à 784 composantes. ``training_data_y`` est un vecteur à 10 composantes contenant le vecteur correspondant au chiffre affiché dans l'image.\n",
    "\n",
    "De façon analogue, ``validation_data_x`` et ``test_data_x`` contiennent 10 000 vecteurs ``x`` correspondant au vecteur image à 784 composante. ``validation_data_y`` et ``test_data_y`` seront deux tableaux de 10 000 lignes contenant l'entier correspondant à l'image.\n",
    "\n",
    "__Aparté__:  NumPy encode les vecteurs en ligne plutôt qu'en colonnes. Ainsi, mathématiquement, un vecteur de 3 composantes s'écrit comme une matrice de 3x1 dimensions en NumPy, et non un vecteur de longueur 3.\n",
    "\n",
    "Exemple: ``a = np.array([0,1,2])`` est un vecteur de dimension (3,) et s'affiche ``[0,1,2]``. Créons une matrice en ajoutant une dimension à ce vecteur avec ``b = a[:,np.newaxis]``. Les dimensions de cette matrice deviennent (3,1) et celui s'affiche\n",
    "```\n",
    "[[0]\n",
    " [1]\n",
    " [2]]\n",
    " ```\n",
    " \n",
    "Ainsi, le nombre 3 qui désignait une colonne dans le vecteur, désigne à présent une ligne dans la matrice. Si cela n'a que peu d'importance si l'on considère ces indices comme des dimensions indépendantes, cela peut prêter à confusion en algèbre linéaire où les indices ont une représentation bien précise en terme de lignes/colonnes.\n",
    "En résumé, représenter rigoureusement les vecteurs en NumPy necessite d'introduire une dimension supplémentaire ce qui alourdit le code. Dans la suite, nous nous conformons à la notation NumPy plutôt que la notation mathématique exacte. Cela implique que tous les vecteurs et tenseurs sont en fait transposés. De plus les produis tensoriels sont inversés: A.B s'écrira np.dot(B,A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dim = training_data[0].shape[1]   #Dimension du vecteur image\n",
    "\n",
    "training_data_x  = np.array(training_data[0])\n",
    "training_data_y = np.eye(10)[training_data[1]]\n",
    "\n",
    "validation_data_x  = np.array(validation_data[0])\n",
    "validation_data_y = validation_data[1]\n",
    "\n",
    "test_data_x  = np.array(test_data[0])\n",
    "test_data_y = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chiffre est 5\n",
      "L'image est:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Le chiffre est\",np.argmax(training_data_y[0]))\n",
    "print(\"L'image est:\")\n",
    "plt.imshow(training_data_x[0,:].reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons définir une liste de nombres `sizes` = [a,b,c...,N] décrivant notre réseau de neurones. a est le nombre de neurones dans la couche d'entrée et N est le nombre de neurones dans la couche de sortie. b,c,... sont les nombres de neurons dans les couches cachées. Par exemple [784, 30, 10] est un réseau à 3 couches présentant 784 neurones d'entrée (les pixels de l'image), 10 neurones de sortie (le vecteur représentant le chiffre) et une couche cachée de 30 neurones.\n",
    "\n",
    "Dans un réseau de neurone, l'activation des neurones dans un couche $l$ dépend de l'activation de tous les neurons de la couche $l-1$ selon:\n",
    "\n",
    "$$z^{l}_j = \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j $$\n",
    "\n",
    "$$a^{l}_j = \\sigma\\left( z^{l}_k \\right)$$\n",
    "\n",
    "Soit, de façon tensorielle:\n",
    "\n",
    "$$ z^l = w^l a^{l-1} + b^l $$\n",
    "\n",
    "$$ a^l = \\sigma( z^l) $$\n",
    "\n",
    "$w_{jk}$ et $b_j$ sont les poids et biais des neurones de la couche $l$ et $\\sigma$ est la fonction d'activation (choisie comme une fonction sigmoïde).\n",
    "\n",
    "Ci-dessous, nous initialisons deux listes contenant (pour chaque couche de neurones) les tenseurs de poids $w_{jk}$ et les vecteurs $b_j$ avec des nombres aléatoire gaussiens (centrés sur 0 et d'écart-type unité). Conformément aux équations précédentes, chaque vecteur de biais présente autant de dimensions/lignes que de neurones dans la couche concernée, et chaque tenseur de poids présente autant de lignes que de neurones dans la couches en cours et autant de colonnes que dans la couche précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les dimensions du tenseur wjk et du vecteur bj dans la couche 1 sont (784, 30)  et  (30,)\n",
      "Les dimensions du tenseur wjk et du vecteur bj dans la couche 2 sont (30, 10)  et  (10,)\n"
     ]
    }
   ],
   "source": [
    "sizes = [N_dim, 30, 10]\n",
    "\n",
    "num_layers = len(sizes)\n",
    "\n",
    "biases = [np.random.randn(x) for x in sizes[1:]]\n",
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "for i in np.arange(len(weights)):\n",
    "    print(\"Les dimensions du tenseur wjk et du vecteur bj dans la couche\", i+1,\n",
    "          \"sont\",np.shape(weights[i]),\" et \", np.shape(biases[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "i = np.linspace(-10, 10, 100)\n",
    "plt.plot(i, sigmoid(i), i, sigmoid_prime(i));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiffre identifié : 5 ; Chiffre réel: 5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Cette fonction calcule la réponse du réseau de neurones pour une image d'entrée \"a\".\n",
    "Elle renvoie un vecteur correspondant au chiffre attendu.\n",
    "'''\n",
    "def feedforward(a):\n",
    "    global biases, weights\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(a, w)+b)\n",
    "    return a\n",
    "\n",
    "# Est-ce que le réseau arrive à reconnaitre le premier chiffre ?\n",
    "calc_vector = feedforward(training_data_x[0])\n",
    "real_vector = training_data_y[0]\n",
    "print(\"Chiffre identifié :\", np.argmax(calc_vector), \"; Chiffre réel:\", np.argmax(real_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y):\n",
    "    \"\"\"Renvoie un tuple ``(nabla_b, nabla_w, err)`` représentant le\n",
    "    le gradient de la fonction de cout C sommé sur toutes les données.\n",
    "    ``nabla_b`` et ``nabla_w`` sont des listes de tableaux numpy \n",
    "    analogues à ``biases`` and ``weights``.\"\"\"\n",
    "    global weights, biases\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # liste contenant toutes les activations par couche\n",
    "    zs = [] # liste contenant les z par couche\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(activation,w)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # Calcul des erreurs de la dernière couche\n",
    "    #err = ((activations[-1] - y)**2).sum(axis=(0,1))\n",
    "    \n",
    "    delta = (activations[-1] - y) * sigmoid_prime(zs[-1])\n",
    "\n",
    "    nabla_b[-1] = delta.sum(axis=0)\n",
    "    nabla_w[-1] = np.dot(activations[-2].transpose(), delta)\n",
    "    \n",
    "    # Retro-propagation\n",
    "    # Graduation negative pour réculer dans le réseau.\n",
    "    for l in range(2, num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(delta, weights[-l+1].transpose()) * sp\n",
    "        nabla_b[-l] = delta.sum(axis=0)\n",
    "        nabla_w[-l] = np.dot(activations[-l-1].transpose(), delta)\n",
    "\n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "def update_weights_biases(x, y, eta):\n",
    "    \"\"\" Ajuste les poids et biais par la méthode du gradient\n",
    "    stochastique dans un mini-lot. ``eta`` est la vitesse d\n",
    "    aprentissage (poids affecté au gradient)\"\"\"\n",
    "    global weights, biases\n",
    "    mb_size = x.shape[0]\n",
    "\n",
    "    nabla_b, nabla_w = backprop(x, y)\n",
    "\n",
    "    weights = [w-(eta/mb_size)*nw for w, nw in zip(weights, nabla_w)]\n",
    "    biases = [b-(eta/mb_size)*nb for b, nb in zip(biases, nabla_b)]\n",
    "\n",
    "def train(train_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    \"\"\"Entraine le réseau de neurone par la méthode du gradient\n",
    "    stochastique par mini-lots. Les données d'entrée sont les \n",
    "    les tableaux MxN_dim où M est le nombre de données expérimentales\n",
    "    et N_dim est la dimension du vecteur image. Epochs est le nombre\n",
    "    d'itération requises pour l'apprentissage. Mini_batch_size est le\n",
    "    nombre de mini-lots. Eta est la vitesse d'aprentissage. Test_data\n",
    "    permet d'évaluer la précision atteinte par le réseau en utilisant\n",
    "    un jeu de données indépendant\"\"\"\n",
    "    global weights, biases, rmse_train, accuracy_train\n",
    "    \n",
    "    (x, y) = train_data\n",
    "    \n",
    "    if test_data:\n",
    "        n_test = len(test_data[1])\n",
    "        rmse_train[0], accuracy_train[0], rmse_test[0], accuracy_test[0] = error_accuracy(x, y, test_data)\n",
    "    else :\n",
    "        rmse_train[0], accuracy_train[0] = error_accuracy(x, y, test_data)\n",
    "         \n",
    "    for j in range(epochs):\n",
    "        s = np.arange(x.shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        x_mb = np.split(x[s], mini_batch_size)\n",
    "        y_mb = np.split(y[s], mini_batch_size)\n",
    "\n",
    "        for xx, yy in zip(x_mb,y_mb):\n",
    "            update_weights_biases(xx, yy, eta)\n",
    "           \n",
    "        \n",
    "\n",
    "        if test_data:\n",
    "            rmse_train[j+1], accuracy_train[j+1], rmse_test[j+1], accuracy_test[j+1] = error_accuracy(x, y, test_data)\n",
    "            print(\"Epoch {} : Training error {:6.4f} (accuracy {:6.4f}%) ; Test error {:6.4f} (accuracy {:6.4f}%)\"\n",
    "                  .format(j+1, rmse_train[j], 100*accuracy_train[j+1], rmse_test[j+1], 100*accuracy_test[j+1]))\n",
    "            \n",
    "        else:\n",
    "            rmse_train[j+1], accuracy_train[j+1] = error_accuracy(x, y, test_data)\n",
    "            print(\"Epoch {} : Training error {:6.4f}  (accuracy {:6.3f}%)\"\n",
    "                  .format(j+1, rmse_train[j+1], 100*accuracy_train[j+1]))\n",
    "\n",
    "\n",
    "def error_accuracy(x, y, test_data=None):\n",
    "    # Calcule la réponse du réseau\n",
    "    a = feedforward(x)\n",
    "    # Evalue l'erreur totale\n",
    "    error = ((a - y)**2).sum(axis=(0,1))\n",
    "    # Combien de chiffre le réseau a-t-il reconnu\n",
    "    result = np.argmax(a, axis=1)\n",
    "    result = (result == np.argmax(y, axis=1)).sum()\n",
    "    \n",
    "    if test_data:\n",
    "        (xt, yt) = test_data\n",
    "        # Calcule la réponse du réseau\n",
    "        at = feedforward(xt)\n",
    "        # Evalue l'erreur totale\n",
    "        yt = np.eye(10)[yt]\n",
    "        error_t = ((at - yt)**2).sum(axis=(0,1))\n",
    "        # Combien de chiffre le réseau a-t-il reconnu\n",
    "        result_t = np.argmax(at, axis=1)\n",
    "        result_t = (result_t == np.argmax(yt, axis=1)).sum()\n",
    "        return error / a.shape[0], result / a.shape[0], error_t / at.shape[0], result_t / at.shape[0]\n",
    "    \n",
    "    else:\n",
    "        return error / a.shape[0], result / a.shape[0]\n",
    "    \n",
    "            \n",
    "def eval_accuracy(test_data_x, test_data_y):\n",
    "    \"\"\"Retourne le nombre de chiffres que le réseau a correctement reconnu\n",
    "    dans un jeu de données indépendant de celui utilisé pour l'entrainement\"\"\"\n",
    "    test_results = [(np.argmax(feedforward(x)), y) for (x, y) in zip(test_data_x, test_data_y)]\n",
    "    return sum(int(x == y) for (x, y) in test_results)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training error 0.2962 (accuracy 79.6660%) ; Test error 0.2872 (accuracy 79.3900%)\n",
      "Epoch 2 : Training error 0.2867 (accuracy 80.1180%) ; Test error 0.2796 (accuracy 79.7700%)\n",
      "Epoch 3 : Training error 0.2786 (accuracy 80.5040%) ; Test error 0.2737 (accuracy 80.0800%)\n",
      "Epoch 4 : Training error 0.2723 (accuracy 80.8060%) ; Test error 0.2675 (accuracy 80.5000%)\n",
      "Epoch 5 : Training error 0.2664 (accuracy 81.0820%) ; Test error 0.2625 (accuracy 80.6900%)\n",
      "Epoch 6 : Training error 0.2610 (accuracy 81.3360%) ; Test error 0.2591 (accuracy 80.8800%)\n",
      "Epoch 7 : Training error 0.2568 (accuracy 81.5920%) ; Test error 0.2550 (accuracy 81.2900%)\n",
      "Epoch 8 : Training error 0.2524 (accuracy 81.8180%) ; Test error 0.2516 (accuracy 81.4400%)\n",
      "Epoch 9 : Training error 0.2488 (accuracy 81.9900%) ; Test error 0.2482 (accuracy 81.6300%)\n",
      "Epoch 10 : Training error 0.2452 (accuracy 82.1920%) ; Test error 0.2455 (accuracy 81.8600%)\n",
      "Epoch 11 : Training error 0.2420 (accuracy 82.3620%) ; Test error 0.2427 (accuracy 82.0200%)\n",
      "Epoch 12 : Training error 0.2388 (accuracy 82.5100%) ; Test error 0.2400 (accuracy 82.1800%)\n",
      "Epoch 13 : Training error 0.2359 (accuracy 82.6540%) ; Test error 0.2380 (accuracy 82.2500%)\n",
      "Epoch 14 : Training error 0.2335 (accuracy 82.7680%) ; Test error 0.2363 (accuracy 82.4600%)\n",
      "Epoch 15 : Training error 0.2314 (accuracy 82.9160%) ; Test error 0.2345 (accuracy 82.4700%)\n",
      "Epoch 16 : Training error 0.2290 (accuracy 82.9820%) ; Test error 0.2322 (accuracy 82.5200%)\n",
      "Epoch 17 : Training error 0.2267 (accuracy 83.1580%) ; Test error 0.2312 (accuracy 82.7300%)\n",
      "Epoch 18 : Training error 0.2250 (accuracy 83.2020%) ; Test error 0.2293 (accuracy 82.6900%)\n",
      "Epoch 19 : Training error 0.2233 (accuracy 83.2960%) ; Test error 0.2278 (accuracy 82.7300%)\n",
      "Epoch 20 : Training error 0.2213 (accuracy 83.4360%) ; Test error 0.2272 (accuracy 82.7600%)\n",
      "Epoch 21 : Training error 0.2199 (accuracy 83.4940%) ; Test error 0.2255 (accuracy 82.7800%)\n",
      "Epoch 22 : Training error 0.2184 (accuracy 83.5460%) ; Test error 0.2238 (accuracy 82.9300%)\n",
      "Epoch 23 : Training error 0.2167 (accuracy 83.6000%) ; Test error 0.2228 (accuracy 83.0800%)\n",
      "Epoch 24 : Training error 0.2154 (accuracy 83.7500%) ; Test error 0.2222 (accuracy 83.0800%)\n",
      "Epoch 25 : Training error 0.2141 (accuracy 83.7140%) ; Test error 0.2208 (accuracy 83.1600%)\n",
      "Epoch 26 : Training error 0.2128 (accuracy 83.8660%) ; Test error 0.2199 (accuracy 83.2700%)\n",
      "Epoch 27 : Training error 0.2114 (accuracy 83.8660%) ; Test error 0.2188 (accuracy 83.3700%)\n",
      "Epoch 28 : Training error 0.2102 (accuracy 83.9580%) ; Test error 0.2181 (accuracy 83.3600%)\n",
      "Epoch 29 : Training error 0.2091 (accuracy 84.0320%) ; Test error 0.2172 (accuracy 83.3800%)\n",
      "Epoch 30 : Training error 0.2084 (accuracy 84.0580%) ; Test error 0.2161 (accuracy 83.4500%)\n",
      "Epoch 31 : Training error 0.2071 (accuracy 84.1800%) ; Test error 0.2154 (accuracy 83.4800%)\n",
      "Epoch 32 : Training error 0.2059 (accuracy 84.2400%) ; Test error 0.2148 (accuracy 83.6500%)\n",
      "Epoch 33 : Training error 0.2050 (accuracy 84.2980%) ; Test error 0.2138 (accuracy 83.5600%)\n",
      "Epoch 34 : Training error 0.2042 (accuracy 84.3400%) ; Test error 0.2134 (accuracy 83.6500%)\n",
      "Epoch 35 : Training error 0.2032 (accuracy 84.4160%) ; Test error 0.2128 (accuracy 83.7100%)\n",
      "Epoch 36 : Training error 0.2022 (accuracy 84.4180%) ; Test error 0.2126 (accuracy 83.6400%)\n",
      "Epoch 37 : Training error 0.2015 (accuracy 84.4960%) ; Test error 0.2116 (accuracy 83.7500%)\n",
      "Epoch 38 : Training error 0.2005 (accuracy 84.5100%) ; Test error 0.2110 (accuracy 83.7300%)\n",
      "Epoch 39 : Training error 0.1997 (accuracy 84.5580%) ; Test error 0.2102 (accuracy 83.7900%)\n",
      "Epoch 40 : Training error 0.1989 (accuracy 84.6080%) ; Test error 0.2104 (accuracy 83.7200%)\n",
      "Epoch 41 : Training error 0.1984 (accuracy 84.6900%) ; Test error 0.2093 (accuracy 83.8200%)\n",
      "Epoch 42 : Training error 0.1975 (accuracy 84.7240%) ; Test error 0.2090 (accuracy 83.9200%)\n",
      "Epoch 43 : Training error 0.1967 (accuracy 84.7680%) ; Test error 0.2080 (accuracy 83.9700%)\n",
      "Epoch 44 : Training error 0.1960 (accuracy 84.8060%) ; Test error 0.2079 (accuracy 83.9400%)\n",
      "Epoch 45 : Training error 0.1954 (accuracy 84.8400%) ; Test error 0.2071 (accuracy 83.9900%)\n",
      "Epoch 46 : Training error 0.1947 (accuracy 84.8720%) ; Test error 0.2068 (accuracy 84.0200%)\n",
      "Epoch 47 : Training error 0.1940 (accuracy 84.9320%) ; Test error 0.2065 (accuracy 84.1100%)\n",
      "Epoch 48 : Training error 0.1934 (accuracy 84.9900%) ; Test error 0.2061 (accuracy 84.0600%)\n",
      "Epoch 49 : Training error 0.1929 (accuracy 84.9720%) ; Test error 0.2055 (accuracy 84.1600%)\n",
      "Epoch 50 : Training error 0.1924 (accuracy 85.0400%) ; Test error 0.2055 (accuracy 84.1400%)\n",
      "Epoch 51 : Training error 0.1920 (accuracy 85.0500%) ; Test error 0.2051 (accuracy 84.1100%)\n",
      "Epoch 52 : Training error 0.1912 (accuracy 85.0480%) ; Test error 0.2046 (accuracy 84.1500%)\n",
      "Epoch 53 : Training error 0.1906 (accuracy 85.0760%) ; Test error 0.2043 (accuracy 84.1600%)\n",
      "Epoch 54 : Training error 0.1900 (accuracy 85.1720%) ; Test error 0.2042 (accuracy 84.1500%)\n",
      "Epoch 55 : Training error 0.1896 (accuracy 85.1780%) ; Test error 0.2035 (accuracy 84.1100%)\n",
      "Epoch 56 : Training error 0.1892 (accuracy 85.2280%) ; Test error 0.2034 (accuracy 84.1800%)\n",
      "Epoch 57 : Training error 0.1885 (accuracy 85.2180%) ; Test error 0.2030 (accuracy 84.2100%)\n",
      "Epoch 58 : Training error 0.1882 (accuracy 85.3180%) ; Test error 0.2023 (accuracy 84.3500%)\n",
      "Epoch 59 : Training error 0.1876 (accuracy 86.6040%) ; Test error 0.2010 (accuracy 85.5600%)\n",
      "Epoch 60 : Training error 0.1858 (accuracy 93.4300%) ; Test error 0.1416 (accuracy 92.2600%)\n",
      "Epoch 61 : Training error 0.1292 (accuracy 93.5720%) ; Test error 0.1316 (accuracy 92.2600%)\n",
      "Epoch 62 : Training error 0.1178 (accuracy 93.6220%) ; Test error 0.1274 (accuracy 92.3000%)\n",
      "Epoch 63 : Training error 0.1135 (accuracy 93.6880%) ; Test error 0.1252 (accuracy 92.3600%)\n",
      "Epoch 64 : Training error 0.1110 (accuracy 93.7220%) ; Test error 0.1236 (accuracy 92.4400%)\n",
      "Epoch 65 : Training error 0.1091 (accuracy 93.7220%) ; Test error 0.1224 (accuracy 92.5000%)\n",
      "Epoch 66 : Training error 0.1079 (accuracy 93.7900%) ; Test error 0.1210 (accuracy 92.5100%)\n",
      "Epoch 67 : Training error 0.1065 (accuracy 93.8740%) ; Test error 0.1199 (accuracy 92.6500%)\n",
      "Epoch 68 : Training error 0.1054 (accuracy 93.8940%) ; Test error 0.1192 (accuracy 92.6200%)\n",
      "Epoch 69 : Training error 0.1044 (accuracy 93.9620%) ; Test error 0.1184 (accuracy 92.7100%)\n",
      "Epoch 70 : Training error 0.1035 (accuracy 93.9800%) ; Test error 0.1177 (accuracy 92.7700%)\n",
      "Epoch 71 : Training error 0.1026 (accuracy 94.0260%) ; Test error 0.1173 (accuracy 92.7500%)\n",
      "Epoch 72 : Training error 0.1018 (accuracy 94.0700%) ; Test error 0.1165 (accuracy 92.9200%)\n",
      "Epoch 73 : Training error 0.1010 (accuracy 94.1060%) ; Test error 0.1162 (accuracy 92.7800%)\n",
      "Epoch 74 : Training error 0.1003 (accuracy 94.1120%) ; Test error 0.1161 (accuracy 92.8800%)\n",
      "Epoch 75 : Training error 0.0997 (accuracy 94.1620%) ; Test error 0.1154 (accuracy 92.9400%)\n",
      "Epoch 76 : Training error 0.0988 (accuracy 94.2240%) ; Test error 0.1151 (accuracy 92.9200%)\n",
      "Epoch 77 : Training error 0.0984 (accuracy 94.2540%) ; Test error 0.1146 (accuracy 93.0500%)\n",
      "Epoch 78 : Training error 0.0977 (accuracy 94.2820%) ; Test error 0.1143 (accuracy 92.9400%)\n",
      "Epoch 79 : Training error 0.0975 (accuracy 94.3640%) ; Test error 0.1139 (accuracy 93.0400%)\n",
      "Epoch 80 : Training error 0.0965 (accuracy 94.3500%) ; Test error 0.1136 (accuracy 93.0600%)\n",
      "Epoch 81 : Training error 0.0960 (accuracy 94.4260%) ; Test error 0.1130 (accuracy 93.1200%)\n",
      "Epoch 82 : Training error 0.0953 (accuracy 94.4180%) ; Test error 0.1130 (accuracy 93.1400%)\n",
      "Epoch 83 : Training error 0.0951 (accuracy 94.4660%) ; Test error 0.1124 (accuracy 93.1700%)\n",
      "Epoch 84 : Training error 0.0944 (accuracy 94.4620%) ; Test error 0.1121 (accuracy 93.1600%)\n",
      "Epoch 85 : Training error 0.0939 (accuracy 94.5140%) ; Test error 0.1115 (accuracy 93.2300%)\n",
      "Epoch 86 : Training error 0.0934 (accuracy 94.5260%) ; Test error 0.1116 (accuracy 93.2800%)\n",
      "Epoch 87 : Training error 0.0930 (accuracy 94.5220%) ; Test error 0.1112 (accuracy 93.4300%)\n",
      "Epoch 88 : Training error 0.0926 (accuracy 94.5800%) ; Test error 0.1113 (accuracy 93.2200%)\n",
      "Epoch 89 : Training error 0.0921 (accuracy 94.5840%) ; Test error 0.1108 (accuracy 93.3500%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 : Training error 0.0915 (accuracy 94.6220%) ; Test error 0.1107 (accuracy 93.3500%)\n",
      "Epoch 91 : Training error 0.0912 (accuracy 94.6480%) ; Test error 0.1102 (accuracy 93.3900%)\n",
      "Epoch 92 : Training error 0.0908 (accuracy 94.6440%) ; Test error 0.1100 (accuracy 93.4500%)\n",
      "Epoch 93 : Training error 0.0905 (accuracy 94.7240%) ; Test error 0.1099 (accuracy 93.3800%)\n",
      "Epoch 94 : Training error 0.0899 (accuracy 94.7140%) ; Test error 0.1094 (accuracy 93.4900%)\n",
      "Epoch 95 : Training error 0.0896 (accuracy 94.7380%) ; Test error 0.1091 (accuracy 93.5400%)\n",
      "Epoch 96 : Training error 0.0892 (accuracy 94.7840%) ; Test error 0.1093 (accuracy 93.4600%)\n",
      "Epoch 97 : Training error 0.0889 (accuracy 94.7900%) ; Test error 0.1090 (accuracy 93.5300%)\n",
      "Epoch 98 : Training error 0.0885 (accuracy 94.8100%) ; Test error 0.1090 (accuracy 93.4600%)\n",
      "Epoch 99 : Training error 0.0883 (accuracy 94.8160%) ; Test error 0.1087 (accuracy 93.4400%)\n",
      "Epoch 100 : Training error 0.0878 (accuracy 94.8600%) ; Test error 0.1084 (accuracy 93.4900%)\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "epochs = 100\n",
    "rate = 5.0\n",
    "\n",
    "rmse_train = np.zeros(epochs+1)\n",
    "accuracy_train = np.zeros(epochs+1)\n",
    "rmse_test = np.zeros(epochs+1)\n",
    "accuracy_test = np.zeros(epochs+1)\n",
    "\n",
    "train((training_data_x, training_data_y), epochs, mini_batch_size, rate, test_data=(test_data_x, test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_train, label='Training accuracy')\n",
    "plt.plot(rmse_train, label='Training error')\n",
    "plt.plot(accuracy_test, label='Testing accuracy')\n",
    "plt.plot(rmse_test, label='Testing error')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "\n",
    "with open('saved_weights', 'wb') as fw:\n",
    "    pickle.dump(weights, fw)\n",
    "with open('saved_biases', 'wb') as fb:\n",
    "    pickle.dump(biases, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "norm_linalg = np.linalg.norm(a)**2\n",
    "norm_sum = (a*a).sum(axis=(0,1))\n",
    "#norm_dot = (np.dot(a,a))\n",
    "print(norm_linalg, norm_sum, norm_dot)\n",
    "np.allclose(norm_linalg, norm_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
